{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "# total_reward = 0\n",
    "\n",
    "# for i_episode in range(3):\n",
    "#     observation = env.reset()\n",
    "#     for t in range(1000000):\n",
    "#         env.render()\n",
    "# #         if t % 50 == 0:\n",
    "# #         print(observation)\n",
    "#         if observation[3] > 0:\n",
    "#             action = 1\n",
    "#         elif observation[3] < 0:\n",
    "#             action = 0\n",
    "#         if abs(observation[3]) < .15 and abs(observation[1]) > .5:\n",
    "#             action = 0 if (action == 1) else 1\n",
    "\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         total_reward += reward\n",
    "# #         print done, reward\n",
    "#         if done:\n",
    "#             print info\n",
    "# #             print(observation)\n",
    "#             print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             break\n",
    "\n",
    "# print(\"average reward is {}\".format(total_reward / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole with GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_shape=[4], activation='relu', kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(6, activation='relu', kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(2, activation='tanh', kernel_initializer='random_uniform'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env = gym.make('CartPoleU-v0')\n",
    "gym.envs.register(\n",
    "    id='CartPoleUV-v0',\n",
    "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "    max_episode_steps=5000,      # MountainCar-v0 uses 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 17:58:05,739] Making new env: CartPoleUV-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPoleUV-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_from_tanh(value):\n",
    "    return 1 if value[0][0]>value[0][1] else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 17:58:08,369] Making new env: CartPoleUV-v0\n",
      "[2017-12-19 17:58:08,381] Creating monitor directory /tmp/cartpole-experiment-1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPoleUV-v0')\n",
    "env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "def start_epoch(population, ):\n",
    "    for agent in population:\n",
    "    \n",
    "        reward_boost = 1\n",
    "        reward_boost_scaler = 2\n",
    "        \n",
    "        max_angle = 0\n",
    "        pos_cicles = 0\n",
    "        neg_cicles = 0\n",
    "        \n",
    "        observation = env.reset()\n",
    "        agent.total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            env.render()\n",
    "\n",
    "            action = get_action_from_tanh(\n",
    "                agent.predict(\n",
    "                    # v1 with only 2 observations. Without convergence\n",
    "#                     np.array([observation[3], observation[1]]).reshape([1, 2])\n",
    "                    # v2 with all 4 observations. convergence.\n",
    "                    np.array(observation).reshape([1, 4])\n",
    "                )\n",
    "            )\n",
    "            #observation[1] angle of plank\n",
    "#             print observation[1] \n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            # reward boost computation         \n",
    "            if observation[1] > 0:\n",
    "                pos_cicles += 1\n",
    "                if neg_cicles <= 2:\n",
    "                    neg_cicles = 0\n",
    "            else:\n",
    "                neg_cicles += 1\n",
    "                if pos_cicles <= 2:\n",
    "                    pos_cicles = 0 \n",
    "            \n",
    "            if neg_cicles > 2 and pos_cicles > 2:\n",
    "                print 'agent boosted for balancing attempt'\n",
    "                reward_boost *= reward_boost_scaler + (10/((neg_cicles + pos_cicles)/2))\n",
    "                pos_cicles, neg_cicles = 0, 0\n",
    "            \n",
    "            max_angle = abs(observation[1]) if abs(observation[1]) > max_angle else max_angle        \n",
    "                    \n",
    "            agent.total_reward += reward * reward_boost\n",
    "        \n",
    "            if done:\n",
    "                \n",
    "                if max_angle < 0.4:\n",
    "                    print 'agent boosted for low max_angles'\n",
    "#                     agent.total_reward *= 3\n",
    "    #             print(observation)\n",
    "#                 print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "                break\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selecting(population, num_of_selected = 2):\n",
    "    return sorted(weighted_population, key=lambda k: k.total_reward, reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossover(agent1, agent2):\n",
    "    a1_w = agent1.get_weights()\n",
    "    a2_w = agent2.get_weights()\n",
    "    \n",
    "    a1_w_n, a2_w_n = [], []\n",
    "\n",
    "    for i, w in enumerate(a1_w):\n",
    "\n",
    "#         if not len(a1_w)-1 == i:\n",
    "        res_shape = w.shape\n",
    "        w = w.flatten()\n",
    "        a2_w[i] = a2_w[i].flatten()\n",
    "\n",
    "        gen_separator = random.randint(1, w.shape[0])\n",
    "\n",
    "        a1_w_n.append(np.concatenate([w[0:gen_separator], a2_w[i][gen_separator:]]).reshape(res_shape))\n",
    "        a2_w_n.append(np.concatenate([a2_w[i][0:gen_separator], w[gen_separator:]]).reshape(res_shape))\n",
    "#         else:\n",
    "#             a1_w_n.append(random.choice([a1_w[i], a2_w[i]]))\n",
    "#             a2_w_n.append(random.choice([a1_w[i], a2_w[i]]))\n",
    "    \n",
    "    return a1_w_n, a2_w_n\n",
    "\n",
    "\n",
    "def mutation(agent):\n",
    "    weights = agent.get_weights()\n",
    "    for i, w in enumerate(weights):\n",
    "#         weights[i] =  w * ((np.random.rand(*w.shape) - 0.5) * 3 + (np.random.rand(*w.shape) - 0.5))\n",
    "        weights[i] =  w * ((np.random.rand(*w.shape) - 0.5)/100)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regenerate_population():\n",
    "    return [get_model() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 17:58:24,619] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.1953.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 17:58:35,231] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.1953.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent boosted for balancing attempt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 17:58:37,311] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.1953.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent boosted for low max_angles\n",
      "New population was created\n",
      "10\n",
      "Population:  2\n",
      "agent boosted for low max_angles\n",
      "New population was created\n",
      "10\n",
      "Population:  3\n",
      "agent boosted for low max_angles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-19 17:58:58,903] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.1953.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent boosted for low max_angles\n",
      "New population was created\n",
      "10\n",
      "Population:  4\n",
      "agent boosted for low max_angles\n",
      "agent boosted for low max_angles\n",
      "New population was created\n",
      "10\n",
      "Population:  5\n",
      "agent boosted for balancing attempt\n",
      "agent boosted for low max_angles\n"
     ]
    }
   ],
   "source": [
    "# Agents creation\n",
    "population = regenerate_population()\n",
    "\n",
    "best_agent = population[0]\n",
    "best_agent.total_reward = -9999\n",
    "\n",
    "p = 1\n",
    "    \n",
    "while True:\n",
    "    print \"Population: \", p\n",
    "    weighted_population = start_epoch(population)\n",
    "    population_sorted_by_results = selecting(weighted_population)\n",
    "    \n",
    "    if best_agent.total_reward <= population_sorted_by_results[0].total_reward:\n",
    "        best_agent = population_sorted_by_results[0]\n",
    "   \n",
    "    if population_sorted_by_results[0].total_reward < 200 or max(env.get_episode_lengths()) < 200:\n",
    "        print 'New population was created'\n",
    "        new_population = regenerate_population()\n",
    "    else:\n",
    "        \n",
    "        new_population = population_sorted_by_results\n",
    "        print ('Same population: reward-',\n",
    "                population_sorted_by_results[0].total_reward,\n",
    "                max(env.get_episode_lengths()))\n",
    "        \n",
    "        # Copy 4th winners #1-4\n",
    "        for a in range(4):\n",
    "            new_population[a].set_weights(population_sorted_by_results[a].get_weights())\n",
    "\n",
    "        # Add 1 child of 2 winners #5\n",
    "        childs_of_winners = crossover(population_sorted_by_results[0], population_sorted_by_results[1])\n",
    "        new_population[4].set_weights(random.choice(childs_of_winners))\n",
    "\n",
    "        # Add 3 child of random 2 winners from 4 of them #6-10\n",
    "        for i in range(5,10):\n",
    "            parent_a = 0\n",
    "            parent_b = parent_a\n",
    "\n",
    "            while parent_a==parent_b:\n",
    "                parent_a = random.choice([0, 1, 2, 3])\n",
    "                parent_b = random.choice([0, 1, 2, 3])\n",
    "\n",
    "            childs = crossover(population_sorted_by_results[parent_a], population_sorted_by_results[parent_b])\n",
    "            new_population[i].set_weights(random.choice(childs))\n",
    "\n",
    "        for agent in new_population[4:]:\n",
    "            agent.set_weights(mutation(agent))\n",
    "\n",
    "    population = new_population\n",
    "    p += 1\n",
    "    print len(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obsevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(env.get_episode_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "population_sorted_by_results[0].total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "population_sorted_by_results[-i-1].set_weights(population_sorted_by_results[-i-1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_agent.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "population_sorted_by_results[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_agent.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_agent.total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
